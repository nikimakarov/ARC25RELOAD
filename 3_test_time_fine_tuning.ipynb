{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-task Test-time fine-tuning for ARC24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will explore a version of test-time fine-tuning that adapts the base model for each task.\n",
    "\n",
    "Instead of fine-tuning in all the test tasks together, I will fine-tune a model for each of the tasks. Hopefully this will bring the improvement that Jack Cole is talking about in the MLST podcast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a dry run, no inference nor installation of packages will be done\n"
     ]
    }
   ],
   "source": [
    "#qwen25-0.5b/8 100split step32k_bs1 8e-5lr_lin 96E\n",
    "n_splits = 100 #2, 4, 10, 20, 50, 100\n",
    "total_train_steps = 32000\n",
    "class cfg:\n",
    "    # Model\n",
    "    model_path = '/kaggle/input/qwen2.5/transformers/0.5b-instruct/1'\n",
    "    input_lora_path = '/kaggle/input/loras/transformers/qwen2.5-0.5b-instruct/8'\n",
    "    prompt_version = 'output-from-examples-v1'\n",
    "    merged_model_path = '/kaggle/tmp/qwen_merged_model'\n",
    "    grid_encoder = 'GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))' #GridCodeBlockEncoder(MinimalGridEncoder())\n",
    "    max_model_len = 10240\n",
    "    # Dataset\n",
    "    dataset_path = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\n",
    "    #dataset_path = '/kaggle/input/arc24-source-code/new_partitions/val_rs7.json'\n",
    "    #dataset_path = 'smaller_val_challenges.json'\n",
    "    split_size = 100//n_splits # How many tasks there would be on each split, use 1 for the canonical single-task test-time fine-tuning\n",
    "    # Fine-tuning params\n",
    "    max_steps = total_train_steps//n_splits\n",
    "    learning_rate = 8e-5 #1e-4 for lora smaller than 512\n",
    "    lr_scheduler_type: str = \"linear\" #linear, constant_with_warmup, cosine, cosine_with_restarts\n",
    "    batch_size = 1\n",
    "    max_seq_len = 5120 #3456, 5120\n",
    "    # Inference params\n",
    "    predictions_per_task = 96 # multiple of 8\n",
    "    inference_timeout = \"12m\" # max inference time per split, I estimate that for 128 preds Qwen-0.5B takes 6 min in the worst case (50 splits)\n",
    "    # Ensemble\n",
    "    ensemble_with_2020: bool = True\n",
    "\n",
    "import os\n",
    "is_dry_run = cfg.dataset_path == '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json' and not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "if is_dry_run:\n",
    "    print('This is a dry run, no inference nor installation of packages will be done')\n",
    "    \n",
    "if int(cfg.input_lora_path.split('/')[-1]) < 18 and cfg.input_lora_path.startswith('/kaggle/input/loras/transformers/qwen2-0.5b'):\n",
    "    assert cfg.prompt_version == 'output-from-examples-v0'\n",
    "else:\n",
    "    assert cfg.prompt_version == 'output-from-examples-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/arc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not is_dry_run:\n",
    "    sys.path.append('/kaggle/input/arc24-source-code')\n",
    "\n",
    "# Configure logging to output to the notebook console\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Launch 2020 solution in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run and cfg.ensemble_with_2020:\n",
    "    print('Launching 2020 solution in the background')\n",
    "    args = [\n",
    "        #'taskset', '-c', '0', apparently this will restrict the job to a single cpu\n",
    "        'python',\n",
    "        '/kaggle/input/arc24-source-code/full_2020_solution.py',\n",
    "        f'--dataset_filepath={cfg.dataset_path}',\n",
    "        '--icecuber_output_filepath=icecuber_submission.json',\n",
    "        '--dsl_output_filepath=submission_program_search.json']\n",
    "    full_2020_solution_process = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install libraries and launch resource monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 μs, sys: 0 ns, total: 1 μs\n",
      "Wall time: 3.1 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not is_dry_run:\n",
    "    !bash /kaggle/input/arc24-source-code/install_libraries.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run:\n",
    "    from arc24.utils import ResourceMonitor\n",
    "    monitor = ResourceMonitor(interval=1)\n",
    "    monitor.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run:\n",
    "    single_task_datasets_path = 'single_task_datasets'\n",
    "    os.makedirs(single_task_datasets_path, exist_ok=True)\n",
    "    with open(cfg.dataset_path, 'r') as f:\n",
    "        items = list(json.load(f).items())\n",
    "    \n",
    "    assert len(items) % cfg.split_size == 0\n",
    "    print(len(items))\n",
    "    for batch_idx in tqdm(range(len(items)//cfg.split_size), desc='Creating single task datasets'):\n",
    "        data = dict(items[batch_idx*cfg.split_size: (batch_idx + 1)*cfg.split_size])\n",
    "        assert len(data) == cfg.split_size\n",
    "        task_id = list(data.keys())[0]\n",
    "        with open(os.path.join(single_task_datasets_path, f'{task_id}.json'), 'w') as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: {single_task_datasets_path}: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! ls -lh {single_task_datasets_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run:\n",
    "    training_datasets_path = 'single_task_training_datasets'\n",
    "    os.makedirs(training_datasets_path, exist_ok=True)\n",
    "    dataset_filepaths = glob.glob(os.path.join(single_task_datasets_path, '*.json'))\n",
    "    for dataset_filepath in tqdm(dataset_filepaths, desc='Creating ttft training datasets'):\n",
    "        !python /kaggle/input/arc24-source-code/create_n-1_dataset.py \\\n",
    "        {dataset_filepath} \\\n",
    "        {os.path.join(training_datasets_path, os.path.basename(dataset_filepath))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test-time fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def clean_train_output_except_adapter(output_dir):\n",
    "    \"\"\"\n",
    "    Max Disk is 57.6GiB, but around 8GiB are already used. \n",
    "    If each checkpoint weights 265M, that would be 26GB for 100 savings. The optimizer was using just 133M.\n",
    "    However on '/kaggle/working/' we can only save 20GB, the run that had the disk error used 50 splits, each split was using around 400M, so it makes sense\n",
    "    \"\"\"\n",
    "    #!rm -rf {output_dir}/runs\n",
    "    !rm {output_dir}/*/*.pth {output_dir}/*/*.pt {output_dir}/*/*.md {output_dir}/*/*.txt {output_dir}/*/*.bin {output_dir}/*/token*\n",
    "    !rm {output_dir}/*/added_tokens.json {output_dir}/*/special_tokens_map.json {output_dir}/*/vocab.json {output_dir}/*/trainer_state.json\n",
    "    \n",
    "if not is_dry_run:\n",
    "    dataset_filepaths = sorted(glob.glob(os.path.join(training_datasets_path, '*.json')))\n",
    "    checkpoints_folder = '/kaggle/tmp/checkpoints' # https://www.kaggle.com/docs/notebooks#technical-specifications\n",
    "    os.makedirs(checkpoints_folder, exist_ok=True)\n",
    "    for dataset_filepath in tqdm(dataset_filepaths, desc='Finetuning models'):\n",
    "        output_dir = os.path.join(checkpoints_folder, os.path.splitext(os.path.basename(dataset_filepath))[0])\n",
    "        !python /kaggle/input/arc24-source-code/fine-tuning.py \\\n",
    "        --model_path={cfg.model_path} \\\n",
    "        --adapter_path={cfg.input_lora_path} \\\n",
    "        --output_dir={output_dir} \\\n",
    "        --train_datasets {dataset_filepath} {cfg.prompt_version} \\\n",
    "        --val_dataset {dataset_filepath} {cfg.prompt_version} \\\n",
    "        --max_steps={cfg.max_steps} \\\n",
    "        --eval_steps={cfg.max_steps*2} \\\n",
    "        --max_seq_len={cfg.max_seq_len} \\\n",
    "        --learning_rate={cfg.learning_rate} \\\n",
    "        --lr_scheduler_type={cfg.lr_scheduler_type} \\\n",
    "        --batch_size={cfg.batch_size} \\\n",
    "        --report_to=tensorboard \\\n",
    "        --grid_encoder=\"{cfg.grid_encoder}\" \\\n",
    "        --remove_train_samples_to_fit_max_seq_len \\\n",
    "        --torch_dtype=float16 \\\n",
    "        --no-verbose\n",
    "        clean_train_output_except_adapter(output_dir)\n",
    "        logging.info(f'Finished fine-tuning for split {dataset_filepaths.index(dataset_filepath) + 1}/{len(dataset_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -lh {checkpoints_folder}/*/checkpoint*/adapter_model.safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Program search \n",
    "\n",
    "https://www.kaggle.com/code/mehrankazeminia/3-arc24-developed-2020-winning-solutions\n",
    "\n",
    "I'm doing program search before inference because I have observed OOM errors when running icecuber solution and inference in parallel. It seems I can fine-tune and run icecuber solution at the same time without any problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run and cfg.ensemble_with_2020:\n",
    "    # old code to call program search dsl sequentially\n",
    "    #!python /kaggle/input/arc24-source-code/program_search_dsl.py \\\n",
    "    #--dataset_filepath={cfg.dataset_path} \\\n",
    "    #--output_filepath=submission_program_search.json\n",
    "    \n",
    "    print('Waiting for icecuber process to end')\n",
    "    full_2020_solution_process.wait()\n",
    "    stdout, stderr = full_2020_solution_process.communicate()\n",
    "    print(\"Script output:\", stdout.decode())\n",
    "    print(\"Script errors:\", stderr.decode())\n",
    "    \n",
    "    #old code to call icecuber solution sequentially\n",
    "    #!python /kaggle/input/arc24-source-code/icecuber_solution.py \\\n",
    "    #--dataset_filepath={cfg.dataset_path} \\\n",
    "    #--output_filepath=icecuber_submission.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if is_dry_run:\n",
    "    with open('submission.json', 'w') as f:\n",
    "        json.dump(dict(dry_run=True), f)\n",
    "else:\n",
    "    inference_path = 'inference'\n",
    "    os.makedirs(inference_path)\n",
    "    os.environ['VLLM_LOGGING_LEVEL'] = 'ERROR'\n",
    "    dataset_filepaths = sorted(glob.glob(os.path.join(single_task_datasets_path, '*.json')))\n",
    "    for dataset_filepath in tqdm(dataset_filepaths):\n",
    "        task_id = os.path.splitext(os.path.basename(dataset_filepath))[0]\n",
    "        checkpoint_path = os.path.join(checkpoints_folder, task_id, f'checkpoint-{cfg.max_steps}')\n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f'Checkpoint path does not exist: {checkpoint_path}')\n",
    "            checkpoint_path = cfg.input_lora_path\n",
    "        \n",
    "        !python /kaggle/input/arc24-source-code/merge_lora.py \\\n",
    "        --base_model_path={cfg.model_path} \\\n",
    "        --lora_path={checkpoint_path} \\\n",
    "        --output_path={cfg.merged_model_path}\n",
    "        \n",
    "        output_filepath = os.path.join(inference_path, f'{task_id}_inference.json')\n",
    "        while not os.path.exists(output_filepath):\n",
    "            ! timeout {cfg.inference_timeout} python /kaggle/input/arc24-source-code/inference.py\\\n",
    "            --model_path={cfg.merged_model_path} \\\n",
    "            --prompt_version={cfg.prompt_version} \\\n",
    "            --dataset={dataset_filepath} \\\n",
    "            --output_filepath={output_filepath} \\\n",
    "            --max_model_len={cfg.max_model_len} \\\n",
    "            --grid_encoder=\"{cfg.grid_encoder}\" \\\n",
    "            --predictions_per_task={cfg.predictions_per_task}  \n",
    "            if not os.path.exists(output_filepath):\n",
    "                print('\\t\\tWARNING, INFERENCE DID TIMEOUT!')\n",
    "                \n",
    "        logging.info(f'Finished inference for split {dataset_filepaths.index(dataset_filepath) + 1}/{len(dataset_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# combine all the predictions into single files\n",
    "if not is_dry_run:\n",
    "    filepaths = glob.glob(os.path.join(inference_path, '*_inference.json'))\n",
    "    solutions = dict()\n",
    "    for filepath in tqdm(filepaths):\n",
    "        with open(filepath, 'r') as f:\n",
    "            solutions.update(json.load(f))\n",
    "    with open('submission_all.json', 'w') as f:\n",
    "        json.dump(solutions, f)\n",
    "\n",
    "    filepaths = glob.glob(os.path.join(inference_path, '*_task_results.json'))\n",
    "    task_results = []\n",
    "    for filepath in tqdm(filepaths):\n",
    "        with open(filepath, 'r') as f:\n",
    "            task_results.extend(json.load(f))\n",
    "    with open('submission_all_task_results.json', 'w') as f:\n",
    "        json.dump(task_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run:\n",
    "    !python /kaggle/input/arc24-source-code/voting.py \\\n",
    "    --input_filepath=submission_all_task_results.json \\\n",
    "    --output_filepath=submission_voting.json  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run and cfg.dataset_path != '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json':\n",
    "    sys.path.append('/kaggle/input/arc24-source-code')\n",
    "    from evaluation import (\n",
    "        load_arc_data_with_solutions, evaluate,\n",
    "        study_effect_of_the_number_of_solutions,\n",
    "        study_attempt_accuracy,\n",
    "        visualize_tasks_and_predictions)\n",
    "    \n",
    "    print('Results with all the predictions')\n",
    "    with open('submission_all.json', 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    data = load_arc_data_with_solutions(cfg.dataset_path)\n",
    "    evaluate(data, solutions)\n",
    "    \n",
    "    study_effect_of_the_number_of_solutions(solutions, data)\n",
    "    visualize_tasks_and_predictions(solutions, data, only_correct=False)\n",
    "    \n",
    "    print('Results from selected 2 attemps')\n",
    "    with open('submission_voting.json', 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    evaluate(data, solutions)\n",
    "    study_attempt_accuracy(solutions, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combine solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run:\n",
    "    if cfg.ensemble_with_2020:\n",
    "        !python /kaggle/input/arc24-source-code/combine_submissions.py \\\n",
    "        --sub_1=submission_program_search.json \\\n",
    "        --sub_2=icecuber_submission.json \\\n",
    "        --output=submission_2020.json \\\n",
    "        --give_preference_to_second_submission_on_second_attempt\n",
    "\n",
    "        !python /kaggle/input/arc24-source-code/combine_submissions.py \\\n",
    "        --sub_1=submission_2020.json \\\n",
    "        --sub_2=submission_voting.json \\\n",
    "        --output=submission.json \\\n",
    "        --give_preference_to_second_submission_on_second_attempt\n",
    "    else:\n",
    "        !cp submission_voting.json submission.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run and cfg.dataset_path != '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json':\n",
    "    print('Results from final submission')\n",
    "    with open('submission.json', 'r') as f:\n",
    "        solutions = json.load(f)\n",
    "    evaluate(data, solutions)\n",
    "    study_attempt_accuracy(solutions, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean():\n",
    "    for filepath in glob.glob('*'):\n",
    "        if filepath == 'submission.json':\n",
    "            continue\n",
    "        if os.path.isdir(filepath):\n",
    "            shutil.rmtree(filepath)\n",
    "        else:\n",
    "            os.remove(filepath)\n",
    "    !rm -rf /kaggle/tmp/*\n",
    "\n",
    "clean()\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Show resources usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not is_dry_run:\n",
    "    monitor.stop()\n",
    "    monitor.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/code/ironbar/fine-tuned-llms-for-arc24-challenge\n",
    "- https://www.kaggle.com/code/ironbar/fine-tune-llm-on-arc\n",
    "- https://www.kaggle.com/code/mehrankazeminia/3-arc24-developed-2020-winning-solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Can I log metrics when not using wandb? Yes, simply by setting the log to tensorboard\n",
    "- [x] Combine with icecuber solution\n",
    "- [x] Group imports\n",
    "- [ ] Assert that the solution has all the predictions, if there is a timeout there might be missing predictions. It seems that VLLM can get stuck sometimes."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "isSourceIdPinned": false,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 5156304,
     "sourceId": 8615222,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5527180,
     "sourceId": 9852521,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 200567623,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 71342,
     "modelInstanceId": 52023,
     "sourceId": 62292,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 71342,
     "modelInstanceId": 52038,
     "sourceId": 62308,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 103906,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 104529,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 107047,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 107048,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 107078,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 107717,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 107718,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 108410,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 108825,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 109428,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 109987,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 111077,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 111313,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 112633,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 77352,
     "sourceId": 113178,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 126273,
     "modelInstanceId": 102059,
     "sourceId": 121302,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 121304,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 121368,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 121561,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 122911,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 122912,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 123108,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 124484,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 124485,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 98761,
     "modelInstanceId": 102061,
     "sourceId": 125288,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
