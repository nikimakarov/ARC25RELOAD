{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835fb45e",
   "metadata": {},
   "source": [
    "# Single-task Test-time fine-tuning for ARC25 (nikimakarov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3fccc",
   "metadata": {},
   "source": [
    "## 0. Configuration and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f262f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÔøΩÔøΩ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è TTFT...\n",
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ kaggle/input/arc-prize-2025/arc-agi_training_challenges.json: 1000 –∑–∞–¥–∞—á\n",
      "üìä –ù–∞–π–¥–µ–Ω–æ 1000 –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–¥–∞—á –∏–∑ 1000\n",
      "üíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: ./small_test_dataset.json\n",
      "‚úÖ –°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å 10 –∑–∞–¥–∞—á–∞–º–∏:\n",
      "  üìã aa18de87: 4 train, 1 test\n",
      "  üìã 22233c11: 3 train, 1 test\n",
      "  üìã 0b17323b: 2 train, 1 test\n",
      "  üìã c4d1a9ae: 2 train, 1 test\n",
      "  üìã 4cd1b7b2: 3 train, 1 test\n",
      "  üìã 44f52bb0: 6 train, 2 test\n",
      "  üìã 3f23242b: 2 train, 1 test\n",
      "  üìã 278e5215: 3 train, 1 test\n",
      "  üìã c35c1b4c: 3 train, 1 test\n",
      "  üìã 2037f2c7: 3 train, 1 test\n",
      "‚úÖ –°–æ–∑–¥–∞–Ω N-1 –¥–∞—Ç–∞—Å–µ—Ç —Å 10 –∑–∞–¥–∞—á–∞–º–∏\n",
      "üíæ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: ./small_test_dataset_n-1.json\n",
      "\n",
      "üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ .:\n",
      "  - small_test_dataset.json (–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç)\n",
      "  - small_test_dataset_n-1.json (N-1 –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è TTFT)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "DEBUG = True  # –ï—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å, —á—Ç–æ–±—ã –∫–æ–¥ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–ª—Å—è\n",
    "\n",
    "if DEBUG:\n",
    "    # Add the arc24-source-code directory to Python path\n",
    "    arc24_path = os.path.abspath('kaggle/input/arc24-source-code')\n",
    "    if arc24_path not in sys.path:\n",
    "        sys.path.append(arc24_path)\n",
    "\n",
    "    # Import the TTFTDatasetCreator directly from the module file\n",
    "    from arc25.create_test_dataset import TTFTDatasetCreator\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã\n",
    "    creator = TTFTDatasetCreator()\n",
    "    original, n_minus_1 = creator.create_ttft_test_datasets(\n",
    "        input_path='kaggle/input/arc-prize-2025/arc-agi_training_challenges.json',\n",
    "        num_tasks=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2b0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#qwen25-0.5b/8 100split step32k_bs1 8e-5lr_lin 96E\n",
    "n_splits = 100 #2, 4, 10, 20, 50, 100\n",
    "total_train_steps = 32000\n",
    "class cfg:\n",
    "    # Model\n",
    "    model_path = '/kaggle/input/qwen2.5/transformers/0.5b-instruct/1'\n",
    "    input_lora_path = '/kaggle/input/loras/transformers/qwen2.5-0.5b-instruct/8'\n",
    "    prompt_version = 'output-from-examples-v1'\n",
    "    merged_model_path = '/kaggle/tmp/qwen_merged_model'\n",
    "    grid_encoder = 'GridShapeEncoder(RowNumberEncoder(MinimalGridEncoder()))' #GridCodeBlockEncoder(MinimalGridEncoder())\n",
    "    max_model_len = 10240\n",
    "    # Dataset\n",
    "    if DEBUG:\n",
    "        dataset_path = 'small_test_dataset_n-1.json'\n",
    "    else:\n",
    "        dataset_path = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'\n",
    "    #dataset_path = '/kaggle/input/arc24-source-code/new_partitions/val_rs7.json'\n",
    "    #dataset_path = 'smaller_val_challenges.json'\n",
    "    split_size = 100//n_splits # How many tasks there would be on each split, use 1 for the canonical single-task test-time fine-tuning\n",
    "    # Fine-tuning params\n",
    "    max_steps = total_train_steps//n_splits\n",
    "    learning_rate = 8e-5 #1e-4 for lora smaller than 512\n",
    "    lr_scheduler_type: str = \"linear\" #linear, constant_with_warmup, cosine, cosine_with_restarts\n",
    "    batch_size = 1\n",
    "    max_seq_len = 5120 #3456, 5120\n",
    "    # Inference params\n",
    "    predictions_per_task = 96 # multiple of 8\n",
    "    inference_timeout = \"12m\" # max inference time per split, I estimate that for 128 preds Qwen-0.5B takes 6 min in the worst case (50 splits)\n",
    "    # Ensemble\n",
    "    ensemble_with_2020: bool = True\n",
    "\n",
    "import os\n",
    "is_dry_run = cfg.dataset_path == '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json' and not os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "if is_dry_run:\n",
    "    print('This is a dry run, no inference nor installation of packages will be done')\n",
    "    \n",
    "if int(cfg.input_lora_path.split('/')[-1]) < 18 and cfg.input_lora_path.startswith('/kaggle/input/loras/transformers/qwen2-0.5b'):\n",
    "    assert cfg.prompt_version == 'output-from-examples-v0'\n",
    "else:\n",
    "    assert cfg.prompt_version == 'output-from-examples-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d1d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c91fcf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikitamakarov/Desktop/ARC_RELOAD/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not is_dry_run:\n",
    "    sys.path.append('kaggle/input/arc24-source-code')\n",
    "\n",
    "# Configure logging to output to the notebook console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e9dda3",
   "metadata": {},
   "source": [
    "## 1. Libraries and resource monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f654992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Installing libraries from wheels...\n",
      "ÔøΩÔøΩ Installing vllm and dependencies...\n",
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement vllm (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for vllm\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "Processing ./kaggle/input/making-wheels-of-necessary-packages-for-vllm/peft-0.12.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in ./.conda/lib/python3.10/site-packages (from peft) (6.0.3)\n",
      "INFO: pip is looking at multiple versions of peft to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch>=1.13.0 (from peft) (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch>=1.13.0\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "Processing ./kaggle/input/making-wheels-of-necessary-packages-for-vllm/trl-0.9.6.dev0-py3-none-any.whl\n",
      "INFO: pip is looking at multiple versions of trl to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch>=1.4.0 (from trl) (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch>=1.4.0\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bitsandbytes (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for bitsandbytes\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "Requirement already satisfied: GPUtil in ./.conda/lib/python3.10/site-packages (1.4.0)\n",
      "Looking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "Processing ./kaggle/input/making-wheels-of-necessary-packages-for-vllm/transformers-4.44.2-py3-none-any.whl\n",
      "Processing ./kaggle/input/making-wheels-of-necessary-packages-for-vllm/filelock-3.15.4-py3-none-any.whl (from transformers)\n",
      "Processing ./kaggle/input/making-wheels-of-necessary-packages-for-vllm/huggingface_hub-0.24.6-py3-none-any.whl (from transformers)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement regex!=2019.12.17 (from transformers) (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for regex!=2019.12.17\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "Processing ./kaggle/input/making-wheels-of-necessary-packages-for-vllm/accelerate-0.33.0-py3-none-any.whl\n",
      "INFO: pip is looking at multiple versions of accelerate to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement numpy<2.0.0,>=1.17 (from accelerate) (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy<2.0.0,>=1.17\u001b[0m\u001b[31m\n",
      "\u001b[0m‚úÖ Core packages installed successfully\n",
      "üì¶ Installing additional packages...\n",
      "Looking in links: kaggle/input/making-wheels-of-necessary-packages-for-vllm\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.10/site-packages (7.0.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement matplotlib (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for matplotlib\u001b[0m\u001b[31m\n",
      "\u001b[0müì¶ Installing fine-tuning dependencies...\n",
      "Requirement already satisfied: wandb in ./.conda/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: tensorboard in ./.conda/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: click>=8.0.1 in ./.conda/lib/python3.10/site-packages (from wandb) (8.3.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.conda/lib/python3.10/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in ./.conda/lib/python3.10/site-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: platformdirs in ./.conda/lib/python3.10/site-packages (from wandb) (4.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./.conda/lib/python3.10/site-packages (from wandb) (6.32.1)\n",
      "Requirement already satisfied: pydantic<3 in ./.conda/lib/python3.10/site-packages (from wandb) (2.11.9)\n",
      "Requirement already satisfied: pyyaml in ./.conda/lib/python3.10/site-packages (from wandb) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.conda/lib/python3.10/site-packages (from wandb) (2.32.5)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./.conda/lib/python3.10/site-packages (from wandb) (2.39.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in ./.conda/lib/python3.10/site-packages (from wandb) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.conda/lib/python3.10/site-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.conda/lib/python3.10/site-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in ./.conda/lib/python3.10/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in ./.conda/lib/python3.10/site-packages (from tensorboard) (1.75.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.conda/lib/python3.10/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in ./.conda/lib/python3.10/site-packages (from tensorboard) (2.2.6)\n",
      "Requirement already satisfied: pillow in ./.conda/lib/python3.10/site-packages (from tensorboard) (11.3.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./.conda/lib/python3.10/site-packages (from tensorboard) (78.1.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.conda/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.conda/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "üéâ Installation complete!\n",
      "CPU times: user 16.4 ms, sys: 11.2 ms, total: 27.6 ms\n",
      "Wall time: 2.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if DEBUG:\n",
    "    # –£–¥–∞–ª—è–µ–º torch –∏ —Å—Ç–∞–≤–∏–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "    !bash ./install_libraries_local.sh\n",
    "else:\n",
    "    # –ï—Å–ª–∏ –Ω–µ dry-run, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∏–∑ kaggle/input\n",
    "    if not is_dry_run:\n",
    "        !bash kaggle/input/arc24-source-code/install_libraries.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb1e31d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: trl in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from trl) (1.9.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: transformers>=4.56.1 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from trl) (4.56.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (2.8.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (0.34.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (0.6.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from transformers>=4.56.1->trl) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/arc/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0526676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.conda/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./.conda/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.6-cp310-cp310-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.3.2-cp310-cp310-macosx_11_0_arm64.whl (253 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.0-cp310-cp310-macosx_10_9_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp310-cp310-macosx_11_0_arm64.whl (65 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]6\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.0 kiwisolver-1.4.9 matplotlib-3.10.6 pyparsing-3.2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63b6c91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_dry_run:    \n",
    "    from arc24.utils import ResourceMonitor    \n",
    "    monitor = ResourceMonitor(interval=1)    \n",
    "    monitor.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c50b0",
   "metadata": {},
   "source": [
    "## 2. Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdd1a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating single task datasets: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 3628.92it/s]\n"
     ]
    }
   ],
   "source": [
    "if not is_dry_run:\n",
    "    single_task_datasets_path = 'single_task_datasets'\n",
    "    os.makedirs(single_task_datasets_path, exist_ok=True)\n",
    "\n",
    "    # –ß–∏—Ç–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "    with open(cfg.dataset_path, 'r') as f:\n",
    "        items = list(json.load(f).items())\n",
    "        assert len(items) % cfg.split_size == 0\n",
    "\n",
    "        print(len(items))\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É —Ç–∞—Å–∫—É\n",
    "        for batch_idx in tqdm(range(len(items) // cfg.split_size), desc='Creating single task datasets'):\n",
    "            data = dict(items[batch_idx * cfg.split_size: (batch_idx + 1) * cfg.split_size])\n",
    "            assert len(data) == cfg.split_size\n",
    "\n",
    "            task_id = list(data.keys())[0]\n",
    "            with open(os.path.join(single_task_datasets_path, f'{task_id}.json'), 'w') as out_f:\n",
    "                json.dump(data, out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae47a6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   2.8K Sep 26 08:23 0b17323b.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   5.4K Sep 26 08:23 2037f2c7.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   2.0K Sep 26 08:23 22233c11.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   2.4K Sep 26 08:23 278e5215.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   2.3K Sep 26 08:23 3f23242b.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   413B Sep 26 08:23 44f52bb0.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   446B Sep 26 08:23 4cd1b7b2.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   1.1K Sep 26 08:23 aa18de87.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   2.0K Sep 26 08:23 c35c1b4c.json\n",
      "-rw-r--r--@ 1 nikitamakarov  staff   1.1K Sep 26 08:23 c4d1a9ae.json\n"
     ]
    }
   ],
   "source": [
    "! ls -lh {single_task_datasets_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20e05e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/44f52bb0.json', output_dataset='single_task_training_datasets/44f52bb0.json')\n",
      "single_task_datasets/44f52bb0.json has 1 tasks\n",
      "New n-1 dataset has 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  10%|‚ñà         | 1/10 [00:00<00:02,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/2037f2c7.json', output_dataset='single_task_training_datasets/2037f2c7.json')\n",
      "single_task_datasets/2037f2c7.json has 1 tasks\n",
      "New n-1 dataset has 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  20%|‚ñà‚ñà        | 2/10 [00:00<00:01,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/278e5215.json', output_dataset='single_task_training_datasets/278e5215.json')\n",
      "single_task_datasets/278e5215.json has 1 tasks\n",
      "New n-1 dataset has 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  30%|‚ñà‚ñà‚ñà       | 3/10 [00:00<00:01,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/c4d1a9ae.json', output_dataset='single_task_training_datasets/c4d1a9ae.json')\n",
      "single_task_datasets/c4d1a9ae.json has 1 tasks\n",
      "New n-1 dataset has 0 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:00<00:01,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/c35c1b4c.json', output_dataset='single_task_training_datasets/c35c1b4c.json')\n",
      "single_task_datasets/c35c1b4c.json has 1 tasks\n",
      "New n-1 dataset has 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:01<00:01,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/22233c11.json', output_dataset='single_task_training_datasets/22233c11.json')\n",
      "single_task_datasets/22233c11.json has 1 tasks\n",
      "New n-1 dataset has 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:01<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/0b17323b.json', output_dataset='single_task_training_datasets/0b17323b.json')\n",
      "single_task_datasets/0b17323b.json has 1 tasks\n",
      "New n-1 dataset has 0 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:01<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/aa18de87.json', output_dataset='single_task_training_datasets/aa18de87.json')\n",
      "single_task_datasets/aa18de87.json has 1 tasks\n",
      "New n-1 dataset has 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:01<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/3f23242b.json', output_dataset='single_task_training_datasets/3f23242b.json')\n",
      "single_task_datasets/3f23242b.json has 1 tasks\n",
      "New n-1 dataset has 0 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:02<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_dataset='single_task_datasets/4cd1b7b2.json', output_dataset='single_task_training_datasets/4cd1b7b2.json')\n",
      "single_task_datasets/4cd1b7b2.json has 1 tasks\n",
      "New n-1 dataset has 1 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating ttft training datasets: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  4.12it/s]\n"
     ]
    }
   ],
   "source": [
    "if not is_dry_run:\n",
    "    training_datasets_path = 'single_task_training_datasets'\n",
    "    os.makedirs(training_datasets_path, exist_ok=True)\n",
    "\n",
    "    dataset_filepaths = glob.glob(os.path.join(single_task_datasets_path, '*.json'))\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º n-1 –¥–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    for dataset_filepath in tqdm(dataset_filepaths, desc='Creating ttft training datasets'):\n",
    "        !python kaggle/input/arc24-source-code/create_n-1_dataset.py \\\n",
    "            {dataset_filepath} \\\n",
    "            {os.path.join(training_datasets_path, os.path.basename(dataset_filepath))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f7f7cb",
   "metadata": {},
   "source": [
    "## 3. Test-time fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0435a243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finetuning models:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/0b17323b/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/0b17323b/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:45,575 - INFO - Finished fine-tuning for split 1/10\n",
      "Finetuning models:  10%|‚ñà         | 1/10 [00:01<00:17,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/2037f2c7/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/2037f2c7/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:46,699 - INFO - Finished fine-tuning for split 2/10\n",
      "Finetuning models:  20%|‚ñà‚ñà        | 2/10 [00:03<00:11,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/22233c11/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/22233c11/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:47,832 - INFO - Finished fine-tuning for split 3/10\n",
      "Finetuning models:  30%|‚ñà‚ñà‚ñà       | 3/10 [00:04<00:09,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/278e5215/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/278e5215/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:48,964 - INFO - Finished fine-tuning for split 4/10\n",
      "Finetuning models:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:05<00:07,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/3f23242b/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/3f23242b/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:50,088 - INFO - Finished fine-tuning for split 5/10\n",
      "Finetuning models:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:06<00:05,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/44f52bb0/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/44f52bb0/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:51,217 - INFO - Finished fine-tuning for split 6/10\n",
      "Finetuning models:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:07<00:04,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/4cd1b7b2/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/4cd1b7b2/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:52,343 - INFO - Finished fine-tuning for split 7/10\n",
      "Finetuning models:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:08<00:03,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/aa18de87/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/aa18de87/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:53,481 - INFO - Finished fine-tuning for split 8/10\n",
      "Finetuning models:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:09<00:02,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/c35c1b4c/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/c35c1b4c/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:54,580 - INFO - Finished fine-tuning for split 9/10\n",
      "Finetuning models:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:10<00:01,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nikitamakarov/Desktop/ARC_RELOAD/kaggle/input/arc24-source-code/fine-tuning.py\", line 15, in <module>\n",
      "    import torch\n",
      "ModuleNotFoundError: No module named 'torch'\n",
      "zsh:1: no matches found: .tmp/checkpoints/c4d1a9ae/*/*.pth\n",
      "zsh:1: no matches found: .tmp/checkpoints/c4d1a9ae/*/added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-26 08:23:55,688 - INFO - Finished fine-tuning for split 10/10\n",
      "Finetuning models: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:12<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.6 ms, sys: 151 ms, total: 215 ms\n",
      "Wall time: 12 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def clean_train_output_except_adapter(output_dir):\n",
    "    \"\"\"\n",
    "    Max Disk is 57.6GiB, but around 8GiB are already used.\n",
    "    If each checkpoint weights 265M, that would be 26GB for 100 savings. \n",
    "    The optimizer was using just 133M.\n",
    "\n",
    "    However, on '/kaggle/working/' we can only save 20GB. \n",
    "    The run that had the disk error used 50 splits, \n",
    "    each split was using around 400M, so it makes sense.\n",
    "    \"\"\"\n",
    "\n",
    "    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –∞–¥–∞–ø—Ç–µ—Ä—ã\n",
    "    !rm \"{output_dir}\"/*/*.pth \"{output_dir}\"/*/*.pt \"{output_dir}\"/*/*.md \"{output_dir}\"/*/*.txt \"{output_dir}\"/*/*.bin \"{output_dir}\"/*/token*\n",
    "    !rm \"{output_dir}\"/*/added_tokens.json \"{output_dir}\"/*/special_tokens_map.json \"{output_dir}\"/*/vocab.json \"{output_dir}\"/*/trainer_state.json\n",
    "\n",
    "\n",
    "if not is_dry_run:\n",
    "    dataset_filepaths = sorted(glob.glob(os.path.join(training_datasets_path, '*.json')))\n",
    "    checkpoints_folder = '.tmp/checkpoints'  # https://www.kaggle.com/docs/notebooks#technical-specifications\n",
    "    os.makedirs(checkpoints_folder, exist_ok=True)\n",
    "\n",
    "    # –ó–∞–ø—É—Å–∫–∞–µ–º fine-tuning –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    for dataset_filepath in tqdm(dataset_filepaths, desc='Finetuning models'):\n",
    "        output_dir = os.path.join(checkpoints_folder, os.path.splitext(os.path.basename(dataset_filepath))[0])\n",
    "\n",
    "        !python kaggle/input/arc24-source-code/fine-tuning.py \\\n",
    "            --model_path={cfg.model_path} \\\n",
    "            --adapter_path={cfg.input_lora_path} \\\n",
    "            --output_dir={output_dir} \\\n",
    "            --train_datasets {dataset_filepath} {cfg.prompt_version} \\\n",
    "            --val_dataset {dataset_filepath} {cfg.prompt_version} \\\n",
    "            --max_steps={cfg.max_steps} \\\n",
    "            --eval_steps={cfg.max_steps*2} \\\n",
    "            --max_seq_len={cfg.max_seq_len} \\\n",
    "            --learning_rate={cfg.learning_rate} \\\n",
    "            --lr_scheduler_type={cfg.lr_scheduler_type} \\\n",
    "            --batch_size={cfg.batch_size} \\\n",
    "            --report_to=tensorboard \\\n",
    "            --grid_encoder=\"{cfg.grid_encoder}\" \\\n",
    "            --remove_train_samples_to_fit_max_seq_len \\\n",
    "            --torch_dtype=float16 \\\n",
    "            --no-verbose\n",
    "\n",
    "        # –û—á–∏—â–∞–µ–º –ª–∏—à–Ω–∏–µ —Ñ–∞–π–ª—ã –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—É—Å–∫–∞\n",
    "        clean_train_output_except_adapter(output_dir)\n",
    "\n",
    "        logging.info(f'Finished fine-tuning for split {dataset_filepaths.index(dataset_filepath) + 1}/{len(dataset_filepaths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c786f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh {checkpoints_folder}/*/checkpoint*/adapter_model.safetensors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
